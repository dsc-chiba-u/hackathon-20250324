#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Officeæ–‡æ›¸ï¼ˆWordã€Excelã€PowerPointï¼‰ç”¨ã®Azure AI Searchã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Blobã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«æ ¼ç´ã•ã‚ŒãŸOfficeæ–‡æ›¸ã‹ã‚‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã¾ã™
"""

import os
import argparse
import json
import requests
import time
import logging
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# API ãƒãƒ¼ã‚¸ãƒ§ãƒ³
API_VERSION = "2024-11-01-Preview"
DEFAULT_OFFICE_INDEX_NAME = "office-docs-index"
DEFAULT_CONTAINER_NAME = "documents"

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def get_search_headers():
    """æ¤œç´¢APIãƒªã‚¯ã‚¨ã‚¹ãƒˆç”¨ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å–å¾—"""
    api_key = os.environ["AZURE_SEARCH_ADMIN_KEY"]
    return {
        "Content-Type": "application/json",
        "api-key": api_key
    }

def create_arg_parser():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’ä½œæˆ"""
    parser = argparse.ArgumentParser(description="Officeæ–‡æ›¸ã‹ã‚‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ")
    parser.add_argument("--index-name", default=DEFAULT_OFFICE_INDEX_NAME, help=f"ä½œæˆã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {DEFAULT_OFFICE_INDEX_NAME}ï¼‰")
    parser.add_argument("--container", default=DEFAULT_CONTAINER_NAME, help=f"æ¤œç´¢å¯¾è±¡ã®ã‚³ãƒ³ãƒ†ãƒŠåï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {DEFAULT_CONTAINER_NAME}ï¼‰")
    parser.add_argument("--prefix", default="contents/office", help="Blobãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼ˆOfficeæ–‡æ›¸ã®æ ¼ç´å ´æ‰€ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: contents/officeï¼‰")
    parser.add_argument("--use-skillset", action="store_true", help="Cognitive Servicesã®ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã™ã‚‹")
    parser.add_argument("--delete-only", action="store_true", help="ãƒªã‚½ãƒ¼ã‚¹ã‚’å‰Šé™¤ã™ã‚‹ã ã‘")
    parser.add_argument("--debug", action="store_true", help="ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã‚’å‡ºåŠ›")
    return parser

def delete_search_resources(endpoint, index_name):
    """æ¤œç´¢ãƒªã‚½ãƒ¼ã‚¹ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼ã€ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆã€ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼‰ã‚’å‰Šé™¤"""
    headers = get_search_headers()
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å‰Šé™¤
    try:
        response = requests.delete(
            f"{endpoint}/indexes/{index_name}?api-version={API_VERSION}",
            headers=headers
        )
        if response.status_code == 204 or response.status_code == 404:
            logger.info(f"ğŸ—‘ï¸  ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{index_name}' ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        else:
            logger.warning(f"âš ï¸  ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{index_name}' ã®å‰Šé™¤ã«å¤±æ•—: {response.text}")
    except Exception as e:
        logger.error(f"âš ï¸ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {str(e)}")

    # ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼ã‚’å‰Šé™¤
    indexer_name = f"{index_name}-indexer"
    try:
        response = requests.delete(
            f"{endpoint}/indexers/{indexer_name}?api-version={API_VERSION}",
            headers=headers
        )
        if response.status_code == 204 or response.status_code == 404:
            logger.info(f"ğŸ—‘ï¸  ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼ '{indexer_name}' ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        else:
            logger.warning(f"âš ï¸  ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼ '{indexer_name}' ã®å‰Šé™¤ã«å¤±æ•—: {response.text}")
    except Exception as e:
        logger.error(f"âš ï¸ ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {str(e)}")

    # ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆã‚’å‰Šé™¤
    skillset_name = f"{index_name}-skillset"
    try:
        response = requests.delete(
            f"{endpoint}/skillsets/{skillset_name}?api-version={API_VERSION}",
            headers=headers
        )
        if response.status_code == 204 or response.status_code == 404:
            logger.info(f"ğŸ—‘ï¸  ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆ '{skillset_name}' ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        else:
            logger.warning(f"âš ï¸  ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆ '{skillset_name}' ã®å‰Šé™¤ã«å¤±æ•—: {response.text}")
    except Exception as e:
        logger.error(f"âš ï¸ ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆå‰Šé™¤ã‚¨ãƒ©ãƒ¼: {str(e)}")

    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’å‰Šé™¤
    datasource_name = f"{index_name}-datasource"
    try:
        response = requests.delete(
            f"{endpoint}/datasources/{datasource_name}?api-version={API_VERSION}",
            headers=headers
        )
        if response.status_code == 204 or response.status_code == 404:
            logger.info(f"ğŸ—‘ï¸  ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ '{datasource_name}' ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
    else:
            logger.warning(f"âš ï¸  ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ '{datasource_name}' ã®å‰Šé™¤ã«å¤±æ•—: {response.text}")
    except Exception as e:
        logger.error(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {str(e)}")

def create_office_index(endpoint, index_name):
    """Officeæ–‡æ›¸ç”¨ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ"""
    headers = get_search_headers()
    
    # Officeæ–‡æ›¸ç”¨ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®šç¾©
    index_definition = {
        "name": index_name,
        "fields": [
            {"name": "id", "type": "Edm.String", "key": True, "searchable": False},
            {"name": "content", "type": "Edm.String", "searchable": True, "filterable": False, "sortable": False, "facetable": False},
            {"name": "merged_content", "type": "Edm.String", "searchable": True, "filterable": False, "sortable": False, "facetable": False},
            {"name": "metadata_storage_name", "type": "Edm.String", "searchable": True, "filterable": True, "sortable": True},
            {"name": "metadata_storage_path", "type": "Edm.String", "searchable": False, "filterable": True, "sortable": True},
            {"name": "metadata_storage_size", "type": "Edm.Int64", "searchable": False, "filterable": True, "sortable": True},
            {"name": "metadata_content_type", "type": "Edm.String", "searchable": True, "filterable": True, "sortable": True},
            {"name": "metadata_author", "type": "Edm.String", "searchable": True, "filterable": True, "sortable": True},
            {"name": "metadata_last_modified", "type": "Edm.DateTimeOffset", "searchable": False, "filterable": True, "sortable": True},
            {"name": "metadata_creation_date", "type": "Edm.DateTimeOffset", "searchable": False, "filterable": True, "sortable": True},
            {"name": "metadata_title", "type": "Edm.String", "searchable": True, "filterable": True, "sortable": True},
            {"name": "language", "type": "Edm.String", "searchable": True, "filterable": True, "sortable": True},
            # æ˜ç¤ºçš„ã«å¤§ããªãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®facetableå±æ€§ã‚’falseã«è¨­å®š
            {"name": "translated_text", "type": "Edm.String", "searchable": True, "filterable": False, "sortable": False, "facetable": False},
            # è¨€èªã”ã¨ã®ç¿»è¨³ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ 
            {"name": "translated_text_ja", "type": "Edm.String", "searchable": True, "filterable": False, "sortable": False, "facetable": False},
            {"name": "translated_text_en", "type": "Edm.String", "searchable": True, "filterable": False, "sortable": False, "facetable": False},
            {"name": "translated_text_fr", "type": "Edm.String", "searchable": True, "filterable": False, "sortable": False, "facetable": False},
            {"name": "translated_text_es", "type": "Edm.String", "searchable": True, "filterable": False, "sortable": False, "facetable": False},
            {"name": "translated_text_de", "type": "Edm.String", "searchable": True, "filterable": False, "sortable": False, "facetable": False},
            {"name": "translated_text_zh_Hans", "type": "Edm.String", "searchable": True, "filterable": False, "sortable": False, "facetable": False},
            {"name": "keyphrases", "type": "Collection(Edm.String)", "searchable": True, "filterable": True, "facetable": True}
            # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¯ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
        ]
    }
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
    try:
    response = requests.put(
            f"{endpoint}/indexes/{index_name}?api-version={API_VERSION}",
        headers=headers,
        json=index_definition
    )
        if response.status_code == 201 or response.status_code == 200:
        logger.info(f"âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{index_name}' ã‚’ä½œæˆã—ã¾ã—ãŸ")
        return True
    else:
            logger.error(f"âš ï¸  ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{index_name}' ã®ä½œæˆã«å¤±æ•—: {response.text}")
            return False
    except Exception as e:
        logger.error(f"âš ï¸ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return False

def create_skillset(endpoint, index_name, cognitive_services_key, cognitive_services_endpoint):
    """Officeæ–‡æ›¸å‡¦ç†ç”¨ã®ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""
    headers = get_search_headers()
    
    skillset_name = f"{index_name}-skillset"

    # Officeæ–‡æ›¸å‡¦ç†ç”¨ã®ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆå®šç¾©
    skillset_definition = {
        "name": skillset_name,
        "description": "Officeæ–‡æ›¸å‡¦ç†ç”¨ã®ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆ",
        "cognitiveServices": {
            "@odata.type": "#Microsoft.Azure.Search.CognitiveServicesByKey",
            "description": "Cognitive Servicesã‚­ãƒ¼ã«ã‚ˆã‚‹çµ±åˆ",
            "key": cognitive_services_key
        },
        "skills": [
            # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²ã‚¹ã‚­ãƒ« - å¤§ããªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†å¯èƒ½ãªã‚µã‚¤ã‚ºã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
            {
                "@odata.type": "#Microsoft.Skills.Text.SplitSkill",
                "context": "/document",
                "textSplitMode": "pages",
                "maximumPageLength": 1000,        # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’å°ã•ãã—ã¦å‡¦ç†ã‚’æœ€é©åŒ–
                "pageOverlapLength": 100,         # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚‚é©åˆ‡ã«èª¿æ•´
                "defaultLanguageCode": "ja",      # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨€èªã‚’è¨­å®š
                "inputs": [
                    {
                        "name": "text",
                        "source": "/document/content"
                    }
                ],
                "outputs": [
                    {
                        "name": "textItems",
                        "targetName": "pages"
                    }
                ]
            },
            # ãƒ†ã‚­ã‚¹ãƒˆã®çµ±åˆ - åˆ†å‰²ã•ã‚ŒãŸãƒšãƒ¼ã‚¸ã‚’å†çµåˆ
            {
                "@odata.type": "#Microsoft.Skills.Text.MergeSkill",
                "context": "/document",
                "insertPreTag": " ",
                "insertPostTag": " ",
                "inputs": [
                    {
                        "name": "text",
                        "source": "/document/content"
                    },
                    {
                        "name": "itemsToInsert",
                        "source": "/document/pages/*"
                    }
                ],
                "outputs": [
                    {
                        "name": "mergedText",
                        "targetName": "merged_content"
                    }
                ]
            },
            # è¨€èªæ¤œå‡ºã‚¹ã‚­ãƒ« - ãƒšãƒ¼ã‚¸ã”ã¨ã«é©ç”¨
            {
                "@odata.type": "#Microsoft.Skills.Text.LanguageDetectionSkill",
                "context": "/document/pages/*",
                "inputs": [
                    {
                        "name": "text",
                        "source": "/document/pages/*"
                    }
                ],
                "outputs": [
                    {
                        "name": "languageCode",
                        "targetName": "language"
                    }
                ]
            },
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå…¨ä½“ã®è¨€èªã‚’æœ€åˆã®ãƒšãƒ¼ã‚¸ã®è¨€èªã‹ã‚‰æ±ºå®š
            {
                "@odata.type": "#Microsoft.Skills.Util.ShaperSkill",
                "context": "/document",
                "inputs": [
                    {
                        "name": "language",
                        "source": "/document/pages/0/language"
                    }
                ],
                "outputs": [
                    {
                        "name": "output",
                        "targetName": "language"
                    }
                ]
            },
            # ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ã‚ºæŠ½å‡ºã‚¹ã‚­ãƒ« - ãƒšãƒ¼ã‚¸ã”ã¨ã«é©ç”¨
            {
                "@odata.type": "#Microsoft.Skills.Text.KeyPhraseExtractionSkill",
                "context": "/document/pages/*",
                "defaultLanguageCode": "ja",
                "inputs": [
                    {
                        "name": "text",
                        "source": "/document/pages/*"
                    }
                ],
                "outputs": [
                    {
                        "name": "keyPhrases",
                        "targetName": "keyPhrasesForPage"
                    }
                ]
            }
            # ç¿»è¨³ã‚¹ã‚­ãƒ«ã¯ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
        ]
    }
    
    # ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆã‚’ä½œæˆ
    try:
    response = requests.put(
            f"{endpoint}/skillsets/{skillset_name}?api-version={API_VERSION}",
        headers=headers,
        json=skillset_definition
    )
        if response.status_code == 201 or response.status_code == 200:
        logger.info(f"âœ… ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆ '{skillset_name}' ã‚’ä½œæˆã—ã¾ã—ãŸ")
        return True
    else:
            logger.error(f"âš ï¸  ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆ '{skillset_name}' ã®ä½œæˆã«å¤±æ•—: {response.text}")
            logger.error(response.text)
            return False
    except Exception as e:
        logger.error(f"âš ï¸ ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return False

def create_datasource(endpoint, index_name, container_name, connection_string, prefix):
    """Blobã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’ä½œæˆ"""
    headers = get_search_headers()

    datasource_name = f"{index_name}-datasource"

    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å®šç¾©
    datasource_definition = {
        "name": datasource_name,
        "type": "azureblob",
        "credentials": {
            "connectionString": connection_string
        },
        "container": {
            "name": container_name,
            "query": prefix
        }
    }

    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’ä½œæˆ
    try:
        response = requests.put(
            f"{endpoint}/datasources/{datasource_name}?api-version={API_VERSION}",
            headers=headers,
            json=datasource_definition
        )
        if response.status_code == 201 or response.status_code == 200:
            logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ '{datasource_name}' ã‚’ä½œæˆã—ã¾ã—ãŸ")
            return True
        else:
            logger.error(f"âš ï¸  ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ '{datasource_name}' ã®ä½œæˆã«å¤±æ•—: {response.text}")
            return False
    except Exception as e:
        logger.error(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return False

def create_indexer(endpoint, index_name, use_skillset=False):
    """Officeæ–‡æ›¸å‡¦ç†ç”¨ã®ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼ã‚’ä½œæˆ"""
    headers = get_search_headers()

    indexer_name = f"{index_name}-indexer"
    datasource_name = f"{index_name}-datasource"
    skillset_name = f"{index_name}-skillset"
    
    # ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼å®šç¾©
    indexer_definition = {
        "name": indexer_name,
        "description": "Officeæ–‡æ›¸å‡¦ç†ç”¨ã®ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼",
        "dataSourceName": datasource_name,
        "targetIndexName": index_name,
        "parameters": {
            "batchSize": 1,  # å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã™ã‚‹ãŸã‚å°ã•ã„ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ä½¿ç”¨
            "configuration": {
                "dataToExtract": "contentAndMetadata",
                "parsingMode": "default"
            }
        },
        "fieldMappings": [
            {
                "sourceFieldName": "metadata_storage_path",
                "targetFieldName": "id",
                "mappingFunction": {
                    "name": "base64Encode"
                }
            },
            {
                "sourceFieldName": "metadata_storage_name",
                "targetFieldName": "metadata_storage_name"
            },
            {
                "sourceFieldName": "metadata_content_type",
                "targetFieldName": "metadata_content_type"
            },
            {
                "sourceFieldName": "metadata_last_modified",
                "targetFieldName": "metadata_last_modified"
            },
            {
                "sourceFieldName": "metadata_author",
                "targetFieldName": "metadata_author"
            },
            {
                "sourceFieldName": "metadata_title",
                "targetFieldName": "metadata_title"
            },
            {
                "sourceFieldName": "metadata_creation_date",
                "targetFieldName": "metadata_creation_date"
            }
        ]
    }

    # ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼å®šç¾©ã«è¿½åŠ 
    if use_skillset:
        indexer_definition["skillsetName"] = skillset_name
        indexer_definition["outputFieldMappings"] = [
            {
                "sourceFieldName": "/document/merged_content",
                "targetFieldName": "content"
            },
            {
                "sourceFieldName": "/document/merged_content",
                "targetFieldName": "merged_content"
            },
            {
                "sourceFieldName": "/document/language",
                "targetFieldName": "language"
            },
            {
                "sourceFieldName": "/document/pages/*/keyPhrasesForPage/*",
                "targetFieldName": "keyphrases"
            }
        ]
    
    # ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼ã‚’ä½œæˆ
    try:
    response = requests.put(
            f"{endpoint}/indexers/{indexer_name}?api-version={API_VERSION}",
        headers=headers,
        json=indexer_definition
    )
        if response.status_code == 201 or response.status_code == 200:
        logger.info(f"âœ… ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼ '{indexer_name}' ã‚’ä½œæˆã—ã¾ã—ãŸ")
        return True
    else:
            logger.error(f"âš ï¸  ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼ '{indexer_name}' ã®ä½œæˆã«å¤±æ•—: {response.text}")
            logger.error(response.text)
            return False
    except Exception as e:
        logger.error(f"âš ï¸ ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return False

def run_indexer(endpoint, index_name):
    """ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼ã‚’å®Ÿè¡Œ"""
    headers = get_search_headers()
    
    indexer_name = f"{index_name}-indexer"
    
    try:
        response = requests.post(
            f"{endpoint}/indexers/{indexer_name}/run?api-version={API_VERSION}",
            headers=headers
        )
    if response.status_code == 202:
        logger.info(f"âœ… ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼ '{indexer_name}' ã‚’å®Ÿè¡Œã—ã¾ã—ãŸ")
        return True
    else:
            logger.warning(f"âš ï¸  ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼ '{indexer_name}' ã®å®Ÿè¡Œã«å¤±æ•—: {response.text}")
            return False
    except Exception as e:
        logger.error(f"âš ï¸ ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
        return False

def update_semantic_configuration(endpoint, index_name):
    """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯è¨­å®šã‚’è©³ç´°ã«èª¿æ•´"""
    headers = get_search_headers()
    
    # APIãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«åˆã‚ã›ãŸã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯è¨­å®š
    semantic_config = {
        "semantic": {
            "configurations": [
                {
                    "name": "office-semantic-config",
                    "prioritizedFields": {
                        "titleField": {
                            "fieldName": "metadata_title"
                        },
                        "prioritizedContentFields": [
                            {
                                "fieldName": "content"
                            },
                            {
                                "fieldName": "translated_text_ja"
                            },
                            {
                                "fieldName": "translated_text_en"
                            }
                        ],
                        "prioritizedKeywordsFields": [
                            {
                                "fieldName": "keyphrases"
                            }
                        ]
                    }
                }
            ]
        }
    }
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯è¨­å®šã‚’æ›´æ–°
    try:
        # ã¾ãšã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æœ€æ–°æƒ…å ±ã‚’å–å¾—
        get_response = requests.get(
            f"{endpoint}/indexes/{index_name}?api-version={API_VERSION}",
            headers=headers
        )
        
        if get_response.status_code != 200:
            logger.error(f"âš ï¸  ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æƒ…å ±ã®å–å¾—ã«å¤±æ•—: {get_response.text}")
            return False
            
        # ç¾åœ¨ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®šç¾©ã‚’å–å¾—
        index_info = get_response.json()
        
        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯è¨­å®šã‚’è¿½åŠ ã¾ãŸã¯æ›´æ–°
        index_info["semantic"] = semantic_config["semantic"]
        
        # æ›´æ–°ã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®šç¾©ã‚’é€ä¿¡
        response = requests.put(
            f"{endpoint}/indexes/{index_name}?api-version={API_VERSION}",
            headers=headers,
            json=index_info
        )
        
        if response.status_code == 200 or response.status_code == 201:
            logger.info(f"âœ… ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯è¨­å®šã‚’ '{index_name}' ã«é©ç”¨ã—ã¾ã—ãŸ")
            return True
        else:
            logger.error(f"âš ï¸  ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯è¨­å®šã®é©ç”¨ã«å¤±æ•—: {response.text}")
            return False
    except Exception as e:
        logger.error(f"âš ï¸ ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯è¨­å®šã‚¨ãƒ©ãƒ¼: {str(e)}")
        return False

def create_semantic_ranking_profile(endpoint, index_name):
    """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ/æ›´æ–°"""
    headers = get_search_headers()
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æƒ…å ±ã‚’å–å¾—
    try:
        response = requests.get(
            f"{endpoint}/indexes/{index_name}?api-version={API_VERSION}",
            headers=headers
        )
        
        if response.status_code != 200:
            logger.error(f"âš ï¸  ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æƒ…å ±ã®å–å¾—ã«å¤±æ•—: {response.text}")
            return False
            
        index_info = response.json()
        scoring_profiles = index_info.get("scoringProfiles", [])
        
        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ 
        hybrid_profile = {
            "name": "vector-hybrid-profile",
            "text": {
                "weights": {
                    "metadata_title": 5,
                    "keyphrases": 3,
                    "content": 2,
                    "translated_text_ja": 2,
                    "translated_text_en": 1
                }
            },
            "functions": [
                {
                    "type": "magnitude",
                    "boost": 2,
                    "fieldName": "keyphrases",
                    "interpolation": "constant"
                }
            ]
        }
        
        # æ—¢å­˜ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«å­˜åœ¨ã—ãªã„å ´åˆã®ã¿è¿½åŠ 
        profile_exists = False
        for profile in scoring_profiles:
            if profile.get("name") == "vector-hybrid-profile":
                profile_exists = True
                break
                
        if not profile_exists:
            scoring_profiles.append(hybrid_profile)
            index_info["scoringProfiles"] = scoring_profiles
        
        # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã—ãŸçŠ¶æ…‹ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ›´æ–°
        response = requests.put(
            f"{endpoint}/indexes/{index_name}?api-version={API_VERSION}",
            headers=headers,
            json=index_info
        )
        
        if response.status_code == 200 or response.status_code == 201:
            logger.info(f"âœ… ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ« 'vector-hybrid-profile' ã‚’ '{index_name}' ã«é©ç”¨ã—ã¾ã—ãŸ")
            return True
        else:
            logger.error(f"âš ï¸  ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®é©ç”¨ã«å¤±æ•—: {response.text}")
        return False
    
    except Exception as e:
        logger.error(f"âš ï¸ ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’è§£æ
    parser = create_arg_parser()
    args = parser.parse_args()

    # ãƒ­ã‚®ãƒ³ã‚°ãƒ¬ãƒ™ãƒ«ã‚’è¨­å®š
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Azure Search ã‚µãƒ¼ãƒ“ã‚¹ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å–å¾—
    search_endpoint = os.environ.get("AZURE_SEARCH_ENDPOINT", "")
    if not search_endpoint:
        logger.error("âš ï¸ ç’°å¢ƒå¤‰æ•° 'AZURE_SEARCH_ENDPOINT' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return

    # ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹ã‚³ãƒ³ãƒ†ãƒŠåã‚’è¨­å®š
    container_name = args.container

    # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æ¥ç¶šæ–‡å­—åˆ—ã‚’å–å¾—
    storage_connection_string = os.environ.get("AZURE_STORAGE_CONNECTION_STRING", "")
    if not storage_connection_string:
        logger.error("âš ï¸ ç’°å¢ƒå¤‰æ•° 'AZURE_STORAGE_CONNECTION_STRING' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return

    # Cognitive Services ã®ã‚­ãƒ¼ã¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å–å¾—
    cognitive_services_key = os.environ.get("AZURE_COGNITIVE_SERVICES_KEY", "")
    cognitive_services_endpoint = os.environ.get("AZURE_COGNITIVE_SERVICES_ENDPOINT", "")
    if args.use_skillset and (not cognitive_services_key or not cognitive_services_endpoint):
        logger.error("âš ï¸ Cognitive Servicesã®è¨­å®šãŒä¸è¶³ã—ã¦ã„ã¾ã™")
        return

    # ãƒªã‚½ãƒ¼ã‚¹ã‚’å‰Šé™¤
    delete_search_resources(search_endpoint, args.index_name)
    
    # å‰Šé™¤ã®ã¿ã®å ´åˆã¯ã“ã“ã§çµ‚äº†
    if args.delete_only:
        return
        
    # ãƒªã‚½ãƒ¼ã‚¹ã‚’ä½œæˆ
    if create_office_index(search_endpoint, args.index_name):
        logger.info(f"ğŸ” ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{args.index_name}' ã‚’ä½œæˆã—ã¾ã—ãŸ")
    else:
        logger.error(f"âš ï¸ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{args.index_name}' ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        return

    # ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆã®ä½œæˆï¼ˆå¿…è¦ãªå ´åˆï¼‰
    if args.use_skillset:
        if create_skillset(search_endpoint, args.index_name, cognitive_services_key, cognitive_services_endpoint):
            logger.info(f"ğŸ§  ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆ '{args.index_name}-skillset' ã‚’ä½œæˆã—ã¾ã—ãŸ")
        else:
            logger.error(f"âš ï¸ ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆ '{args.index_name}-skillset' ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            return

    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®ä½œæˆ
    if create_datasource(search_endpoint, args.index_name, container_name, storage_connection_string, args.prefix):
        logger.info(f"ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ '{args.index_name}-datasource' ã‚’ä½œæˆã—ã¾ã—ãŸ")
    else:
        logger.error(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ '{args.index_name}-datasource' ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        return

    # ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼ã®ä½œæˆ
    if create_indexer(search_endpoint, args.index_name, args.use_skillset):
        logger.info(f"ğŸ”„ ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼ '{args.index_name}-indexer' ã‚’ä½œæˆã—ã¾ã—ãŸ")
    else:
        logger.error(f"âš ï¸ ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼ '{args.index_name}-indexer' ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
        
    # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯è¨­å®šã®æ›´æ–°
    if update_semantic_configuration(search_endpoint, args.index_name):
        logger.info(f"ğŸ§  ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯è¨­å®šã‚’ '{args.index_name}' ã«é©ç”¨ã—ã¾ã—ãŸ")
    else:
        logger.warning(f"âš ï¸ ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯è¨­å®šã®é©ç”¨ã«å•é¡ŒãŒã‚ã‚Šã¾ã—ãŸ")
        
    # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
    if create_semantic_ranking_profile(search_endpoint, args.index_name):
        logger.info(f"âš–ï¸ ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ '{args.index_name}' ã«é©ç”¨ã—ã¾ã—ãŸ")
    else:
        logger.warning(f"âš ï¸ ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®é©ç”¨ã«å•é¡ŒãŒã‚ã‚Šã¾ã—ãŸ")

    logger.info(f"âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å: {args.index_name}")
    logger.info(f"ğŸ” ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼ã®çŠ¶æ…‹ã‚’ç¢ºèªã™ã‚‹ã«ã¯ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
    logger.info(f"   python index_manager.py --action check --index-name {args.index_name}")

if __name__ == "__main__":
    exit(main()) 