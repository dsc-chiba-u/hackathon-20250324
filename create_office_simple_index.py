#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ã‚·ãƒ³ãƒ—ãƒ«ãªOfficeæ–‡æ›¸ç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
================================================

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯Azure Blobã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«æ ¼ç´ã•ã‚ŒãŸOfficeæ–‡æ›¸ï¼ˆWordã€Excelã€PowerPointï¼‰ã‹ã‚‰
æœ€å°é™ã®æ©Ÿèƒ½ã§Azure AI Searchã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã¾ã™ã€‚

ä¸»ãªæ©Ÿèƒ½:
- Officeæ–‡æ›¸ã‹ã‚‰ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–
- åŸºæœ¬çš„ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã€ä½œæˆæ—¥ã€ä½œæˆè€…ãªã©ï¼‰ã®å–å¾—
- 1ã¤ã®å˜ç´”ãªã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆï¼ˆè¨€èªæ¤œå‡ºï¼‰ã®ã‚µãƒ³ãƒ—ãƒ«

ä½¿ç”¨æ–¹æ³•:
```bash
python create_office_simple_index.py --index-name "my-simple-index" --container "documents"
```

å¿…è¦ãªç’°å¢ƒå¤‰æ•°:
- AZURE_SEARCH_ENDPOINT: Azure AI Searchã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
- AZURE_SEARCH_ADMIN_KEY: ç®¡ç†è€…ã‚­ãƒ¼
- AZURE_STORAGE_CONNECTION_STRING: Blobã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã®æ¥ç¶šæ–‡å­—åˆ—
- AZURE_COGNITIVE_SERVICES_KEY: Cognitive Servicesã®ã‚­ãƒ¼ï¼ˆè¨€èªæ¤œå‡ºã‚¹ã‚­ãƒ«ç”¨ï¼‰
- AZURE_COGNITIVE_SERVICES_ENDPOINT: Cognitive Servicesã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

å‚è€ƒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:
- https://learn.microsoft.com/ja-jp/azure/search/search-howto-indexing-azure-blob-storage
"""

import os
import argparse
import logging
import requests
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# å®šæ•°
API_VERSION = "2024-11-01-Preview"
DEFAULT_INDEX_NAME = "office-simple-index"
DEFAULT_CONTAINER_NAME = "documents"

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def get_headers():
    """Azure AI Search APIç”¨ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å–å¾—"""
    api_key = os.environ.get("AZURE_SEARCH_ADMIN_KEY")
    if not api_key:
        raise ValueError("ç’°å¢ƒå¤‰æ•° 'AZURE_SEARCH_ADMIN_KEY' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    return {
        "Content-Type": "application/json",
        "api-key": api_key
    }

def parse_arguments():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’ãƒ‘ãƒ¼ã‚¹"""
    parser = argparse.ArgumentParser(description="ã‚·ãƒ³ãƒ—ãƒ«ãªOfficeæ–‡æ›¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ")
    parser.add_argument("--index-name", default=DEFAULT_INDEX_NAME,
                        help=f"ä½œæˆã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {DEFAULT_INDEX_NAME}ï¼‰")
    parser.add_argument("--container", default=DEFAULT_CONTAINER_NAME,
                        help=f"æ¤œç´¢å¯¾è±¡ã®ã‚³ãƒ³ãƒ†ãƒŠåï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {DEFAULT_CONTAINER_NAME}ï¼‰")
    parser.add_argument("--prefix", default="",
                        help="Blobãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼ˆæŒ‡å®šã—ãŸãƒ‘ã‚¹ä»¥ä¸‹ã®ã¿ã‚’å¯¾è±¡ã«ã™ã‚‹ï¼‰")
    parser.add_argument("--delete-only", action="store_true",
                        help="æ—¢å­˜ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’å‰Šé™¤ã™ã‚‹ã®ã¿")
    parser.add_argument("--use-skillset", action="store_true",
                        help="è¨€èªæ¤œå‡ºã®ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã™ã‚‹")
    return parser.parse_args()

def delete_resources(endpoint, index_name):
    """æ¤œç´¢ãƒªã‚½ãƒ¼ã‚¹ã‚’å‰Šé™¤"""
    headers = get_headers()
    resources = [
        ("indexes", index_name, "ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹"),
        ("indexers", f"{index_name}-indexer", "ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼"),
        ("skillsets", f"{index_name}-skillset", "ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆ"),
        ("datasources", f"{index_name}-datasource", "ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹")
    ]
    
    for resource_type, resource_name, display_name in resources:
        try:
            response = requests.delete(
                f"{endpoint}/{resource_type}/{resource_name}?api-version={API_VERSION}",
                headers=headers
            )
            if response.status_code in [204, 404]:
                logger.info(f"ğŸ—‘ï¸ {display_name} '{resource_name}' ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            else:
                logger.warning(f"âš ï¸ {display_name} '{resource_name}' ã®å‰Šé™¤ã«å¤±æ•—: {response.text}")
        except Exception as e:
            logger.error(f"âš ï¸ {display_name}å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {str(e)}")

def create_index(endpoint, index_name):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªOfficeæ–‡æ›¸ç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ"""
    headers = get_headers()
    
    # ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®šç¾©
    index_definition = {
        "name": index_name,
        "fields": [
            {"name": "id", "type": "Edm.String", "key": True, "searchable": False},
            {"name": "content", "type": "Edm.String", "searchable": True, "filterable": False, "sortable": False},
            {"name": "metadata_storage_name", "type": "Edm.String", "searchable": True, "filterable": True, "sortable": True},
            {"name": "metadata_storage_path", "type": "Edm.String", "searchable": False, "filterable": True, "sortable": False},
            {"name": "metadata_content_type", "type": "Edm.String", "searchable": True, "filterable": True, "sortable": True},
            {"name": "metadata_last_modified", "type": "Edm.DateTimeOffset", "searchable": False, "filterable": True, "sortable": True},
            {"name": "metadata_author", "type": "Edm.String", "searchable": True, "filterable": True, "sortable": True},
            {"name": "metadata_title", "type": "Edm.String", "searchable": True, "filterable": True, "sortable": True},
            {"name": "language", "type": "Edm.String", "searchable": True, "filterable": True, "sortable": True}
        ]
    }
    
    try:
        response = requests.put(
            f"{endpoint}/indexes/{index_name}?api-version={API_VERSION}",
            headers=headers,
            json=index_definition
        )
        if response.status_code in [200, 201]:
            logger.info(f"âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{index_name}' ã‚’ä½œæˆã—ã¾ã—ãŸ")
            return True
        else:
            logger.error(f"âš ï¸ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{index_name}' ã®ä½œæˆã«å¤±æ•—: {response.text}")
            return False
    except Exception as e:
        logger.error(f"âš ï¸ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return False

def create_skillset(endpoint, index_name, cognitive_services_key):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªè¨€èªæ¤œå‡ºã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""
    headers = get_headers()
    
    skillset_name = f"{index_name}-skillset"

    # è¨€èªæ¤œå‡ºã®ã¿ã®ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆ
    skillset_definition = {
        "name": skillset_name,
        "description": "è¨€èªæ¤œå‡ºã®ã¿ã®ç°¡æ˜“ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆ",
        "cognitiveServices": {
            "@odata.type": "#Microsoft.Azure.Search.CognitiveServicesByKey",
            "description": "Cognitive Services",
            "key": cognitive_services_key
        },
        "skills": [
            # è¨€èªæ¤œå‡ºã‚¹ã‚­ãƒ«
            {
                "@odata.type": "#Microsoft.Skills.Text.LanguageDetectionSkill",
                "context": "/document",
                "inputs": [
                    {
                        "name": "text",
                        "source": "/document/content"
                    }
                ],
                "outputs": [
                    {
                        "name": "languageCode",
                        "targetName": "language"
                    }
                ]
            }
        ]
    }
    
    try:
        response = requests.put(
            f"{endpoint}/skillsets/{skillset_name}?api-version={API_VERSION}",
            headers=headers,
            json=skillset_definition
        )
        if response.status_code in [200, 201]:
            logger.info(f"âœ… ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆ '{skillset_name}' ã‚’ä½œæˆã—ã¾ã—ãŸ")
            return True
        else:
            logger.error(f"âš ï¸ ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆ '{skillset_name}' ã®ä½œæˆã«å¤±æ•—: {response.text}")
            return False
    except Exception as e:
        logger.error(f"âš ï¸ ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return False

def create_datasource(endpoint, index_name, container_name, connection_string, prefix):
    """Blobã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’ä½œæˆ"""
    headers = get_headers()

    datasource_name = f"{index_name}-datasource"

    # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å®šç¾©
    datasource_definition = {
        "name": datasource_name,
        "type": "azureblob",
        "credentials": {
            "connectionString": connection_string
        },
        "container": {
            "name": container_name
        }
    }
    
    # ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãŒã‚ã‚‹å ´åˆã¯è¿½åŠ 
    if prefix:
        datasource_definition["container"]["query"] = prefix

    try:
        response = requests.put(
            f"{endpoint}/datasources/{datasource_name}?api-version={API_VERSION}",
            headers=headers,
            json=datasource_definition
        )
        if response.status_code in [200, 201]:
            logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ '{datasource_name}' ã‚’ä½œæˆã—ã¾ã—ãŸ")
            return True
        else:
            logger.error(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ '{datasource_name}' ã®ä½œæˆã«å¤±æ•—: {response.text}")
            return False
    except Exception as e:
        logger.error(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return False

def create_indexer(endpoint, index_name, use_skillset):
    """ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼ã‚’ä½œæˆ"""
    headers = get_headers()

    indexer_name = f"{index_name}-indexer"
    datasource_name = f"{index_name}-datasource"
    skillset_name = f"{index_name}-skillset"
    
    # ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼å®šç¾©
    indexer_definition = {
        "name": indexer_name,
        "description": "Officeæ–‡æ›¸ã®åŸºæœ¬ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼",
        "dataSourceName": datasource_name,
        "targetIndexName": index_name,
        "parameters": {
            "configuration": {
                "dataToExtract": "contentAndMetadata",
                "parsingMode": "default"
            }
        },
        "fieldMappings": [
            {
                "sourceFieldName": "metadata_storage_path",
                "targetFieldName": "id",
                "mappingFunction": {
                    "name": "base64Encode"
                }
            }
        ]
    }

    # ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ
    if use_skillset:
        indexer_definition["skillsetName"] = skillset_name
        indexer_definition["outputFieldMappings"] = [
            {
                "sourceFieldName": "/document/language",
                "targetFieldName": "language"
            }
        ]
    
    try:
        response = requests.put(
            f"{endpoint}/indexers/{indexer_name}?api-version={API_VERSION}",
            headers=headers,
            json=indexer_definition
        )
        if response.status_code in [200, 201]:
            logger.info(f"âœ… ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼ '{indexer_name}' ã‚’ä½œæˆã—ã¾ã—ãŸ")
            return True
        else:
            logger.error(f"âš ï¸ ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼ '{indexer_name}' ã®ä½œæˆã«å¤±æ•—: {response.text}")
            return False
    except Exception as e:
        logger.error(f"âš ï¸ ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return False

def run_indexer(endpoint, index_name):
    """ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼ã‚’å®Ÿè¡Œ"""
    headers = get_headers()
    
    indexer_name = f"{index_name}-indexer"
    
    try:
        response = requests.post(
            f"{endpoint}/indexers/{indexer_name}/run?api-version={API_VERSION}",
            headers=headers
        )
        if response.status_code == 202:
            logger.info(f"âœ… ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼ '{indexer_name}' ã‚’å®Ÿè¡Œã—ã¾ã—ãŸ")
            return True
        else:
            logger.warning(f"âš ï¸ ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼ '{indexer_name}' ã®å®Ÿè¡Œã«å¤±æ•—: {response.text}")
            return False
    except Exception as e:
        logger.error(f"âš ï¸ ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è§£æ
    args = parse_arguments()
    
    # ç’°å¢ƒå¤‰æ•°ã®å–å¾—
    search_endpoint = os.environ.get("AZURE_SEARCH_ENDPOINT")
    if not search_endpoint:
        logger.error("âš ï¸ ç’°å¢ƒå¤‰æ•° 'AZURE_SEARCH_ENDPOINT' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return 1
        
    storage_connection_string = os.environ.get("AZURE_STORAGE_CONNECTION_STRING")
    if not storage_connection_string:
        logger.error("âš ï¸ ç’°å¢ƒå¤‰æ•° 'AZURE_STORAGE_CONNECTION_STRING' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return 1
    
    cognitive_services_key = os.environ.get("AZURE_COGNITIVE_SERVICES_KEY")
    if args.use_skillset and not cognitive_services_key:
        logger.error("âš ï¸ ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆä½¿ç”¨æ™‚ã¯ç’°å¢ƒå¤‰æ•° 'AZURE_COGNITIVE_SERVICES_KEY' ãŒå¿…è¦ã§ã™")
        return 1
    
    # æ—¢å­˜ãƒªã‚½ãƒ¼ã‚¹ã®å‰Šé™¤
    delete_resources(search_endpoint, args.index_name)
    
    # å‰Šé™¤ã®ã¿ã®å ´åˆã¯ã“ã“ã§çµ‚äº†
    if args.delete_only:
        return 0
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ
    if not create_index(search_endpoint, args.index_name):
        return 1
    
    # ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆã®ä½œæˆï¼ˆå¿…è¦ãªå ´åˆï¼‰
    if args.use_skillset:
        if not create_skillset(search_endpoint, args.index_name, cognitive_services_key):
            return 1
    
    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®ä½œæˆ
    if not create_datasource(search_endpoint, args.index_name, args.container, 
                             storage_connection_string, args.prefix):
        return 1
    
    # ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼ã®ä½œæˆ
    if not create_indexer(search_endpoint, args.index_name, args.use_skillset):
        return 1
    
    # ã‚¤ãƒ³ãƒ‡ã‚¯ã‚µãƒ¼ã®å®Ÿè¡Œ
    run_indexer(search_endpoint, args.index_name)
    
    logger.info(f"âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆãƒ—ãƒ­ã‚»ã‚¹ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å: {args.index_name}")
    return 0

if __name__ == "__main__":
    exit(main())
